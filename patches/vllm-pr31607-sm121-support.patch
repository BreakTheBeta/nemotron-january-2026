# vLLM PR #31607 - Add SM 12.1 support for Blackwell GB10
# https://github.com/vllm-project/vllm/pull/31607
#
# This patch fixes FP8 model loading on NVIDIA DGX Spark (GB10, sm_121a).
# Without this patch, the V1 engine hangs during kernel initialization
# because CUTLASS operations aren't available for sm_121.
#
# Patch 1: _custom_ops.py - Add exception handling for CUTLASS operations
# Patch 2: mxfp4.py - Extend device capability range to include sm_121
# Patch 3: w8a8_utils.py - Wrap CUTLASS flags with exception handling
# Patch 4: matcher_utils.py - Add hasattr checks for torch custom ops

--- a/vllm/_custom_ops.py
+++ b/vllm/_custom_ops.py
@@ -430,11 +430,21 @@ def cutlass_scaled_mm(a: torch.Tensor,


 def cutlass_scaled_mm_supports_fp8(cuda_device_capability: int) -> bool:
-    return torch.ops._C.cutlass_scaled_mm_supports_fp8(cuda_device_capability)
+    try:
+        return torch.ops._C.cutlass_scaled_mm_supports_fp8(cuda_device_capability)
+    except Exception as e:
+        import warnings
+        warnings.warn(f"cutlass_scaled_mm_supports_fp8 unavailable: {e}")
+        return False


 def cutlass_scaled_mm_supports_block_fp8(cuda_device_capability: int) -> bool:
-    return torch.ops._C.cutlass_scaled_mm_supports_block_fp8(
-        cuda_device_capability)
+    try:
+        return torch.ops._C.cutlass_scaled_mm_supports_block_fp8(
+            cuda_device_capability)
+    except Exception as e:
+        import warnings
+        warnings.warn(f"cutlass_scaled_mm_supports_block_fp8 unavailable: {e}")
+        return False


--- a/vllm/model_executor/layers/quantization/mxfp4.py
+++ b/vllm/model_executor/layers/quantization/mxfp4.py
@@ -93,7 +93,7 @@ class MXFP4LinearMethod(LinearMethodBase):
         # MXFP4 kernels only work on H100/H200 (sm_90)
         # Extend range to include Blackwell (sm_120, sm_121)
         return (
-            and (9, 0) <= current_platform.get_device_capability() < (11, 0)
+            and (9, 0) <= current_platform.get_device_capability() <= (12, 1)
         )

@@ -155,7 +155,7 @@ class MXFP4MoEMethod(FusedMoEMethodBase):
         # MXFP4 kernels only work on H100/H200 (sm_90)
         # Extend range to include Blackwell (sm_120, sm_121)
         return (
-            and (9, 0) <= current_platform.get_device_capability() < (11, 0)
+            and (9, 0) <= current_platform.get_device_capability() <= (12, 1)
         )

--- a/vllm/model_executor/layers/quantization/utils/w8a8_utils.py
+++ b/vllm/model_executor/layers/quantization/utils/w8a8_utils.py
@@ -20,8 +20,14 @@ from vllm.model_executor.layers.quantization.utils.quant_utils import (
 from vllm.model_executor.parameter import (ModelWeightParameter,
                                            PerTensorScaleParameter)

-CUTLASS_FP8_SUPPORTED = cutlass_fp8_supported()
-CUTLASS_BLOCK_FP8_SUPPORTED = cutlass_block_fp8_supported()
+try:
+    CUTLASS_FP8_SUPPORTED = cutlass_fp8_supported()
+except Exception:
+    CUTLASS_FP8_SUPPORTED = False
+try:
+    CUTLASS_BLOCK_FP8_SUPPORTED = cutlass_block_fp8_supported()
+except Exception:
+    CUTLASS_BLOCK_FP8_SUPPORTED = False

--- a/vllm/compilation/matcher_utils.py
+++ b/vllm/compilation/matcher_utils.py
@@ -1,4 +1,4 @@
-SILU_MUL_OP = torch.ops._C.silu_and_mul.default
+SILU_MUL_OP = torch.ops._C.silu_and_mul.default if hasattr(torch.ops._C, "silu_and_mul") else None

 # After the line defining kFp8StaticTensorSym, etc., update the dictionary:
 {
-    kFp8StaticTensorSym: torch.ops._C.static_scaled_fp8_quant.default,
-    kFp8DynamicTensorSym: torch.ops._C.dynamic_scaled_fp8_quant.default,
-    kFp8DynamicTokenSym: torch.ops._C.dynamic_per_token_scaled_fp8_quant.default,
+    kFp8StaticTensorSym: torch.ops._C.static_scaled_fp8_quant.default if hasattr(torch.ops._C, "static_scaled_fp8_quant") else None,
+    kFp8DynamicTensorSym: torch.ops._C.dynamic_scaled_fp8_quant.default if hasattr(torch.ops._C, "dynamic_scaled_fp8_quant") else None,
+    kFp8DynamicTokenSym: torch.ops._C.dynamic_per_token_scaled_fp8_quant.default if hasattr(torch.ops._C, "dynamic_per_token_scaled_fp8_quant") else None,
 }
